<!DOCTYPE html>
<!--[if IE 8]> 
<html lang="en" class="ie8">
   <![endif]-->
   <!--[if IE 9]> 
   <html lang="en" class="ie9">
      <![endif]-->
      <!--[if !IE]><!--> 
      <html lang="en">
         <!--<![endif]-->
         <head>
            <title>Guochen Yu</title>
            <!-- Meta -->
            <meta charset="utf-8">
            <meta http-equiv="X-UA-Compatible" content="IE=edge,Chrome=1">
            <!-- <meta name="viewport" content="width=device-width, initial-scale=1.0">-->
            <meta name="description" content="Guochen Yu's homepage">
            <link rel="shortcut icon" href="assets/images/log.png">
            <link href='https://fonts.googleapis.com/css?family=Roboto:400,500,400italic,300italic,300,500italic,700,700italic,900,900italic' rel='stylesheet' type='text/css'>
            <!-- Global CSS -->
            <link rel="stylesheet" href="assets/css/bootstrap.min.css">
            <link rel="stylesheet" href="assets/css/font-awesome/css/font-awesome.min.css">
            <link rel="stylesheet" href="assets/css/main.css">
            <script src="bootstrap/js/bootstrap.min.js"></script>
            <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
            <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
            <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
            <![endif]-->
            <script>
               (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
               (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
               m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
               })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
               
               ga('create', 'UA-88572407-1', 'auto');
               ga('send', 'pageview');
            </script>
         </head>
         <body>
            <div class="container">
            <div class="row">
               <div class='row'>
                  <hr>
                  <div class='col-xs-5'>
                     <hr>
                     <div class='photo'>
                        <img src="assets/images/cv.jpg" alt="photo" width="60px"/>
                     </div>
                  </div>
                  <div class='col-xs-6'>
                     <h3>
                        Guochen Yu 余果宸
                     </h3>
                     <p>
					 <p>
					 <p>
					 <td align="left"><p>audio processing researcher at Kuaishou<br />
					 <a href="http://www.cuc.edu.cn/">Ph.D. Student at State Key Laboratory of Media Convergence and Communication, Communication University of China</a>,
						Beijing, China <br /> 
					<a href="http://www.ioa.ac.cn/">Ph.D. Student at Key Laboratory of Noise and Vibration Research, Institute of Acoustics, Chinese Academy of Sciences</a>, 
						Beijing, China <br />
					Email: yuguochen@kuaishou.com <br />
                     </p>
                     <h3 style="padding-top:-1px"></h3>
                     <a href="mailto: yuguochen@kuaishou.com"><i class="fa fa-envelope"></i> &nbsp; yuguochen@kuaishou.com</a>
                     &nbsp; &nbsp; &nbsp;
                     <a href="https://github.com/yuguochencuc" target="_blank"><i class="fa fa-github-square"></i> &nbsp; Github</a>
                     &nbsp; &nbsp; &nbsp;
                     <a href="https://scholar.google.com/citations?hl=en&user=BeYoFhMAAAAJ" target="_blank"><i class="fa fa-globe"></i> &nbsp; Google Scholar</a>
                  </div>
               </div>
               <hr>
			   <div class='researchInt'>
			  <h3 style="padding-top:-5px">Short Biography</h3>
				<p>I am currently a researcher at kuaishou, working on Large speech model and speech signal processing. I recieved my Ph.D. degree from Communication University of China and also jointly at Institute of Acoustics, Chinese Academy of Sciences (IACAS) supervised by <a href="http://people.ucas.ac.cn/~cszheng">Prof.Chengshi Zheng</a>.</a>.  
				<p>Before that, I received my bachelor's degree from the school of information and communication engineering at Communication University of China in 2017. I was a visiting student at mmlab of the school of information engineering at the Chinese University of Hong Kong (CUHK) in 2019.
				</a>. 
				<p>In my normal life, I like different kinds of sports, including basektball, swimming and Athletics. I used to be the captain of the basketball team of the Communication University of China, and led the team to achieve the top eight results in Beijing. </p>
			   </div>
			   
               <div class='researchInt'>
                  <h3 style="padding-top:-5px">Research Interest</h3>
                  <p>My research interests include large speech model (Zero-shot TTS and GPT-4O) and the front-end techniques for speech and audio, such as speech enhancement and speech signal improvement.</p>
                  </div>
                  <hr>
               <h3>
                  <a name='publications and Manuscripts'></a> Publications and Manuscripts
               </h3>
               
               <h4>
                  <a name='2023'></a> 2023
             </h4>
		 
	<div class="media">
		  <div class="media-body">
		     <p class="media-heading">
			<strong>FSI-Net: A dual-stage Full-Sub-band Integration Network for full-band Speech Enhancement</strong><br />
			<strong>Guochen Yu</strong>, Andong Li, Hui Wang, Wenzhe Liu, Chengshi Zheng<br />
			<a href="https:"></a> Submitted to Appiled Acoustics (<strong>Appiled Acoustics</strong>) <br />
		     </p>
		  </div>
	       </div>	
		 

		 
	<div class="media">
		  <div class="media-body">
		     <p class="media-heading">
			<strong>A General Deep Learning Speech Enhancement Framework Motivated by Taylor's Theorem</strong><br />
			Andong Li, <strong>Guochen Yu</strong>,  Chengshi Zheng, Wenzhe Liu, Xiaodong Li<br />
			<a href="https:"></a> IEEE/ACM Transactions on Audio, Speech, and Language Processing, (<strong>IEEE/ACM TASLP</strong>) <br />
			<a href="https://arxiv.org/abs/2211.16764">[Paper]</a>
		     </p>
		  </div>
	       </div>	
		 		 

	<div class="media">
		  <div class="media-body">
		     <p class="media-heading">
			<strong>TaylorBeamixer: Learning Taylor-Inspired All-Neural Multi-Channel Speech Enhancement from Beam-Space Dictionary Perspective</strong><br />
			Andong Li, <strong>Guochen Yu</strong>, Wenzhe Liu, Xiaodong Li, Chengshi Zheng<br />
			<a href="https:"></a> Submitted to Annual Conference of the International Speech Communication Association (<strong>Interspeech 2023</strong>)<br />
			<a href="https://arxiv.org/abs/2211.12024">[Paper]</a>
		     </p>
		  </div>
	       </div>	
		 
               <h4>
                  <a name='2022'></a> 2022
             </h4>	
		 
	<div class="media">
		  <div class="media-body">
		     <p class="media-heading">
			<strong>DBT-Net: Dual-branch federative magnitude and phase estimation with attention-in-attention transformer for monaural speech enhancement</strong><br />
			<strong>Guochen Yu</strong>, Andong Li, Hui Wang, Yutian Wang, Yuxuan Ke, Chengshi Zheng<br />
			<a href="https:"></a> IEEE/ACM Transactions on Audio, Speech, and Language Processing, (<strong>IEEE/ACM TASLP</strong>) <br />
			<a href="https://arxiv.org/abs/2202.07931">[Paper]</a>
			<a href="https://github.com/yuguochencuc/DBT-Net">[Demopage]</a>
		     </p>
		  </div>
	       </div>	

	<div class="media">
                  <div class="media-body">
                     <p class="media-heading">
                        <strong>TaylorBeamformer: Learning All-Neural Multi-Channel Speech Enhancement from Taylor's Approximation Theory</strong><br />
                        Andong Li, <strong>Guochen Yu</strong>, Chengshi Zheng, Xiaodong Li<br />
                        <a href="https:"></a> Annual Conference of the International Speech Communication Association (<strong>Interspeech2022</strong>)<br />
			 <a href="https://arxiv.org/abs/2203.07195">[Paper]</a>
			<a href="https://github.com/Andong-Li-speech/TaylorBeamformer">[Code]</a>
                     </p>
                  </div>
               </div>	
		 
	               <div class="media">
                  <div class="media-body">
                     <p class="media-heading">
                        <strong>TMGAN-PLC: Audio Packet Loss Concealment using Temporal Memory Generative Adversarial Network</strong><br />
                        Yuansheng Guan, <strong>Guochen Yu</strong>, Andong Li, Chengshi Zheng, Jie Wang<br />
                        <a href="https:"></a> Annual Conference of the International Speech Communication Association (<strong>Interspeech2022</strong>)<br />
                     	<a href="https://github.com/Guanyuansheng/TMGAN-for-2022-PLC-Challenge">[Webpage]</a>
			  </p>
                  </div>
               </div>	
		 
	       	<div class="media">
                  <div class="media-body">
                     <p class="media-heading">
                        <strong>Filtering and Refining: A Collaborative-Style Framework for Single-Channel Speech Enhancement</strong><br />
                        Andong Li, <strong> Guochen Yu</strong>, Chengshi Zheng, Xiaodong Li<br />
                        <a href="https:"></a> IEEE/ACM Transactions on Audio, Speech, and Language Processing (<strong>IEEE/ACM TASLP</strong>) <br />
			<a href="https://github.com/Andong-Li-speech/G2Net">[Code]</a>
			<a href="https://github.com/Andong-Li-speech/G2Net">[Demopage]</a>
                     </p>
                  </div>
               </div>		 
		 
	        <div class="media">
                  <div class="media-body">
                     <p class="media-heading">
                        <strong>Taylor, Can You Hear Me Now? A Taylor-Unfolding Framework for Monaural Speech Enhancement Theory</strong><br />
                        Andong Li, Shan You, <strong>Guochen Yu</strong>, Chengshi Zheng, Xiaodong Li<br />
                        <a href="https:"></a> Accepted by 31st International Joint Conference on Artificial Intelligence (<strong>IJCAI-2022 Oral(top 25%)</strong>)<br />
			<a href="https://arxiv.org/abs/2205.00206">[Paper]</a>
			<a href="https://github.com/Andong-Li-speech/TaylorSENet">[Code]</a>
                     </p>
                  </div>
               </div>		 
		 
             <div class="media">
               <div class="media-body">
                  <p class="media-heading">
                     <strong>Joint Magnitude Estimation and Phase Recovery Using Cycle-in-Cycle GAN for Non-Parallel Speech Enhancement</strong><br />
                     <strong>Guochen Yu</strong>, Andong Li, Yutian Wang, Chengshi Zheng, Hui Wang, Qin Zhang<br />
                     <a href="https:"></a> in IEEE International Conference on Acoustics, Speech, & Signal Processing (<strong>ICASSP2022</strong>)<br/>
			<a href="https://ieeexplore.ieee.org/abstract/document/9747267">[Paper]</a>
			<a href="https://github.com/yuguochencuc/CinCGAN-SE">[Demopage]</a>
                  </p>
               </div>
            </div>

             <div class="media">
               <div class="media-body">
                  <p class="media-heading">
                     <strong>Dual-Branch Attention-in-Attention Transformer for Speech Enhancement</strong><br />
                     <strong>Guochen Yu</strong>, Andong Li, Yinuo Guo, Yutian Wang, Chengshi Zheng, Hui Wang<br />
                     <a href="https:"></a> in IEEE International Conference on Acoustics, Speech, & Signal Processing (<strong>ICASSP2022</strong>)<br/>
			<a href="https://ieeexplore.ieee.org/abstract/document/9746273">[Paper]</a>
			<a href="https://github.com/yuguochencuc/DB-AIAT">[Code]</a>
                  </p>
               </div>
            </div>
            
	    

             <div class="media">
               <div class="media-body">
                  <p class="media-heading">
                     <strong>DMF-Net: A decoupling-style multi-band fusion model for real-time full-band speech enhancement</strong><br />
                     <strong>Guochen Yu</strong>, Yuansheng Guan, Weixin Meng, Chengshi Zheng, Hui Wang<br />
			  <a href="https:"></a> Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (<strong>APSIPA-ASC 2022</strong>)<br />
			<a href="https://arxiv.org/abs/2203.00472">[Paper]</a>   
                  </p>
               </div>
            </div>

             <div class="media">
               <div class="media-body">
                  <p class="media-heading">
                     <strong>Optimizing Shoulder to Shoulder: A Coordinated Sub-Band Fusion Model for Real-Time Full-Band Speech Enhancement</strong><br />
                     <strong>Guochen Yu</strong>, Andong Li, Wenzhe Liu, Chengshi Zheng, Yutian Wang, Hui Wang<br />
                     <a href="https:"></a> International Symposium on Chinese Spoken Language Processing 2022 (<strong>ISCSLP 2022</strong>) <br/>
			<a href="https://arxiv.org/abs/2203.16033">[Paper]</a>
			<a href="https://github.com/yuguochencuc/SF-Net">[Code]</a>
			  <a href="https://yuguochencuc.github.io/sfnet_demo/">[Demopage]</a>
			  
                  </p>
               </div>
            </div>		    
	    

		    
		    
			   <h4>
                    <a name='2021'></a> 2021
               </h4>
               <div class="media">
                  <div class="media-body">
                     <p class="media-heading">
                        <strong>CycleGAN-based Non-parallel Speech Enhancement with an Adaptive Attention-in-attention Mechanism</strong><br />
                        <strong>Guochen Yu</strong>, Yutian Wang, Chengshi Zheng, Hui Wang, Qin Zhang<br />
                        <a href="https:"></a> in Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (<strong>APSIPA-ASC 2021</strong>)<br />
			<a href="https://ieeexplore.ieee.org/document/9689669">[Paper]</a>
                     </p>
                  </div>
               </div> 

               <div class="media">
                  <div class="media-body">
                     <p class="media-heading">
                        <strong>A Two-stage Complex Network using Cycle-consistent Generative Adversarial Networks for Speech Enhancement</strong><br />
                        <strong>Guochen Yu</strong>, Yutian Wang, Hui Wang, Qin Zhang, Chengshi Zheng,<br />
                        Speech Communication, 2021 <br />
			 <a href="https://www.sciencedirect.com/science/article/abs/pii/S0167639321000972">[Paper]</a>
                     </p>
                  </div>
               </div>


               <div class="media">
                  <div class="media-body">
                     <p class="media-heading">
                        <strong>A Simultaneous Denoising and Dereverberation Framework with Target Decoupling</strong><br />
                        Andong Li, Wenzhe Liu, Xiaoxue Luo, <strong>Guochen Yu</strong>, Chengshi Zheng, Xiaodong Li<br />
                        in Annual Conference of the International Speech Communication Association (<strong>Interspeech2021</strong>)<br />
                        <a href="https://arxiv.org/abs/2106.12743">[Paper]</a>
			<a href="https://github.com/Andong-Li-speech/3rd-DNS-Challenge-samples">[Demopage]</a>
                     </p>
                  </div>
               </div>

			   <h4>
                    <a name='2018-2020'></a> 2018-2020
			</h4>	  
               <div class="media">
                  <div class="media-body">
                     <p class="media-heading">
                        <strong>Improved Relativistic Cycle-consistent GAN with Dilated Residual Network and Multi-Attention for Speech Enhancement</strong><br />
                        Yutian Wang*, <strong>Guochen Yu*</strong>, Jingling Wang, Hui Wang, Qin Zhang <strong>(*co-first author, student first author)</strong><br />
                        IEEE ACCESS, 2020<br />
                        <a href="https://ieeexplore.ieee.org/document/9216160">[Paper]</a>
                     </p>
                  </div>
               </div>
		    
		    
               <div class="media">
                  <div class="media-body">
                     <p class="media-heading">
                        <strong>Multi-category MIDI music generation based on LSTM Generative adversarial network</strong><br />
                        Yutian Wang*, <strong>Guochen Yu*</strong>, JuanJuan Cai, Hui Wang <strong>(*co-first author, student first author)</strong><br />
                        In 2018 International Conference on Modeling, Simulation and Computing Science,<strong>MSCS 2018</strong><br />
                        <a href="https:"></a>
                     </p>
                  </div>
               </div>

              
               <h3>
                  <a name='selected-awards'></a> Selected Awards
               </h3>
               <div class="media">
                  <div class="media-body">
                     <p class="media-heading">	
                     [2021/03] INTERSPEECH 2022 Audio Deep Packet Loss Concealment Challenge (Rank 1 in non-causal methods. Best scores in PLCMOS, CMOS and DNSMOS). Check the <a href="https://www.microsoft.com/en-us/research/academic-program/audio-deep-packet-loss-concealment-challenge-interspeech-2022/">website</a><br />
                     [2022/01] ICASSP 2022 Deep Noise Suppression (DNS) Challenge in Track 1 non-personalized DNS. (Rank 9/27, Background noise suprression Rank 3). Check the <a href="https://www.microsoft.com/en-us/research/academic-program/deep-noise-suppression-challenge-icassp-2022/">website</a><br />
                     [2021/03] Champion of INTERSPEECH 2021 Deep Noise Suppression (DNS) Challenge in 1st track. Check the <a href="https://www.microsoft.com/en-us/research/academic-program/deep-noise-suppression-challenge-interspeech-2021/">website</a><br />
                     [2020/10] Champion of ICASSP 2021 Deep Noise Suppression (DNS) Challenge in 1st track. Check the <a href="https://www.microsoft.com/en-us/research/dns-challenge/icassp2021/home">website</a><br />
                     [2019/09] 'Honor Prize' of IEEE ISI-World Cup 2019 in Mission.1: Company Investment Value Evaluation
                     
                     </p>
                  </div>
               </div>
               <hr>               
            </div>
            </div>

            <!--//main-body-->
            <hr>
            <footer class="footer">
               <div class="container">
                  <small class="copyright"> 2022 Guochen Yu</small>
               </div>
               <!--//container-->
            <!--/footer-->
            <!--//footer-->
            <!--div style="display:inline-block;width:200px;">
               <script type="text/javascript" src="//rf.revolvermaps.com/0/0/8.js?i=5ekw1not1zt&amp;m=8&amp;c=ff0000&amp;cr1=ffffff&amp;f=arial&amp;l=0" async="async"></script>
            </div-->
		<a href="https://clustrmaps.com/site/1bnt0" title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=MGC_qVsuS4LcpCtKLJkcbwYEZRrlGW1ia6JsU0KuaM8&cl=ffffff"></a>
         </body>
      </html>
